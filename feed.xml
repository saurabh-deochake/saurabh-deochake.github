<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://saurabhdeochake.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://saurabhdeochake.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-28T01:02:54+00:00</updated><id>https://saurabhdeochake.com/feed.xml</id><title type="html">blank</title><subtitle>Hi, this is Saurabh. </subtitle><entry><title type="html">Trends in Computer Science and Information Technology Research</title><link href="https://saurabhdeochake.com/blog/2023/trends-in-research/" rel="alternate" type="text/html" title="Trends in Computer Science and Information Technology Research"/><published>2023-11-25T00:00:00+00:00</published><updated>2023-11-25T00:00:00+00:00</updated><id>https://saurabhdeochake.com/blog/2023/trends-in-research</id><content type="html" xml:base="https://saurabhdeochake.com/blog/2023/trends-in-research/"><![CDATA[<p>Diverse domains present both challenges and opportunities for researchers and practitioners in the rapidly evolving landscape of information technology and computer science. Our recent paper, published in the International Journal of Computer Applications, aims to provide a concise summary of key insights from a variety of domains, ranging from Software-Defined Networking (SDN) security concerns to the transformative potential of Decision Support Systems (DSS) in agriculture. To give a summary of the paper in short, I decided to write this post.</p> <h1 id="sdn-and-arp-attacks-in-the-digital-realm">SDN and ARP Attacks in the Digital Realm</h1> <p>The importance of strong security measures has been emphasized in the realm of Software-Defined Networking. ARP attacks pose a significant threat, highlighting the need for advanced security protocols to protect network infrastructure and ensure data integrity.</p> <h1 id="revolutionary-trends-in-digital-learning">Revolutionary Trends in Digital Learning</h1> <p>Digital learning has undergone a profound transformation, fueled by innovations such as personalized learning via AI and Machine Learning (ML). Platforms such as Duolingo demonstrate real-world AI implementations, personalizing the learning experience and providing students with tailored recommendations based on their progress and preferences. The paper discusses how recents research trends in digital learning are shaping up the future scope of this area.</p> <h1 id="protection-of-privacy-in-telecare-medicine-information-systems-tmis">Protection of Privacy in Telecare Medicine Information Systems (TMIS)</h1> <p>Telecare Medicine Information Systems (TMIS) have emerged as a modern healthcare solution, offering convenient services via digital mediums. To protect sensitive medical data, privacy-preserving protocols are critical in this context, with a focus on efficient authentication frameworks, cryptographic techniques, anonymization methods, and secure data sharing. This area is something I am interesting in learning in depth since the privacy aspects of the TMIS area is something that will be crucial in this era of interconnected world.</p> <h1 id="cloud-computing-workflow-scheduling-efficiency">Cloud Computing Workflow Scheduling Efficiency</h1> <p>Because of the dynamic nature of cloud computing, efficient workflow scheduling is required. Novel scheduling algorithms, energy-efficient strategies, cost-cutting techniques, and machine learning integration are shaping the future of cloud resource allocation, performance, and sustainability. I believe that this is another area that will continue to see cutting-edge research, especially as the generative AI field advances rapidly.</p> <h1 id="chatgpt-driven-it-infrastructure-administration">ChatGPT-Driven IT Infrastructure Administration</h1> <p>The integration of ChatGPT and IT infrastructure management ushers in a paradigm shift and this area is something I am super excited about. ChatGPT serves as a catalyst for more intuitive interactions, efficient troubleshooting, and collaborative decision-making among IT professionals, from automating routine tasks to improving information security practices. From infrastructure automation to internal service desk for the engineers, generative AI and LLMs will transform the way we do IT infrastructure administration. I am currently working on a paper to study the effectiveness of ChatGPT-like LLM models in IT administration.</p> <h1 id="automated-e-commerce-negotiation">Automated E-Commerce Negotiation</h1> <p>Intelligent software agents are redefining E-Commerce negotiation processes. Automated negotiation systems use artificial intelligence, machine learning, and blockchain technology to streamline negotiations between buyers and sellers, improving efficiency, transparency, and reliability in online transactions. I have been working in this area of research for several years and I believe that with the recent advancements in machine learning area, automated negotiation will see more transformative research.</p> <h1 id="agriculture-decision-support-systems">Agriculture Decision Support Systems</h1> <p>Decision Support Systems (DSS) that facilitate informed decision-making benefit grape growers and farmers in general. Trends include IoT and data analytics integration, AI for yield prediction, mobile applications, and the use of big data and cloud computing to improve precision agriculture. This is another area I intend to expand and learn in depth in the future.</p> <p>The technological frontiers discussed in the paper span a wide range of domains, each with its own set of challenges and promising avenues for future research. I truly believe that the evolving landscape of IT and computer science holds the key to shaping a more connected, secure, and efficient future, from securing digital networks to providing farmers with intelligent decision-making tools.</p> <h1 id="download">Download</h1> <p>You can download and review the Open Access version of the paper at <a href="http://dx.doi.org/10.5120/ijca2023923183">IJCA Journal</a> or <a href="https://ssrn.com/abstract=4560193">Elsevier SSRN Electronic Journal</a>.</p>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="ai"/><category term="chatgpt"/><category term="economics"/><summary type="html"><![CDATA[Evaluative review of research trends in Computer Science and Information Technology]]></summary></entry><entry><title type="html">Cloud Cost Optimization - Strategies and Case Studies</title><link href="https://saurabhdeochake.com/blog/2023/cost-optimization/" rel="alternate" type="text/html" title="Cloud Cost Optimization - Strategies and Case Studies"/><published>2023-07-30T00:00:00+00:00</published><updated>2023-07-30T00:00:00+00:00</updated><id>https://saurabhdeochake.com/blog/2023/cost-optimization</id><content type="html" xml:base="https://saurabhdeochake.com/blog/2023/cost-optimization/"><![CDATA[<p>In the rapidly evolving world of cloud computing, cost optimization has become a critical aspect for businesses and organizations seeking to efficiently leverage the benefits of cloud services. I’ve been working on a paper titled “Cloud Cost Optimization: A Comprehensive Review of Strategies and Case Studies” for several months now, which delves into the multifaceted realm of cloud cost optimization, exploring various strategies and presenting real-world case studies to demonstrate their practical applications.</p> <p>The paper begins with a thorough overview of the challenges that organizations face when managing cloud costs. It emphasizes the growing complexity of cloud pricing models, as well as the need for businesses to strike a delicate balance between resource utilization and budget constraints.</p> <p>To address these issues, the paper thoroughly examines a variety of cloud cost optimization strategies. These strategies include both technical and non-technical approaches, providing readers with a comprehensive perspective. The paper examines a wide range of methods for achieving cost-effectiveness in cloud deployments, from leveraging Reserved Instances and Spot Instances to optimizing resource allocation and implementing auto-scaling mechanisms.</p> <p>Furthermore, the paper uses real-world case studies to demonstrate how these strategies can be successfully implemented in a variety of industries and scenarios. These case studies are useful examples of how organizations from various industries have optimized their cloud spending and achieved significant cost savings.</p> <p>The paper provides readers with a comprehensive toolkit for developing tailored cloud cost optimization strategies by combining insights from industry experts, research studies, and practical experiences. To keep up with the dynamic cloud landscape, it emphasizes the importance of continuous monitoring, analysis, and adaptation.</p> <p>Overall, “Cloud Cost Optimization: A Comprehensive Review of Strategies and Case Studies” is an excellent resource for cloud architects, IT managers, and decision-makers navigating the complexities of cloud cost management. The paper not only provides a thorough understanding of cost optimization techniques, but it also inspires readers to maximize their return on investment by realizing the full potential of cloud services.</p> <h1 id="download">Download</h1> <p>You can download and review the preprint PDF on this paper at <a href="https://arxiv.org/abs/2307.12479">arXiv</a> or <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4519171">Elsevier SSRN Electronic Journal</a>.</p>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="gcp"/><category term="aws"/><category term="azure"/><category term="finops"/><category term="economics"/><summary type="html"><![CDATA[Cloud Cost Optimization - Strategies and Case Studies]]></summary></entry><entry><title type="html">LXC Containers Not Authorized to Change Passwords</title><link href="https://saurabhdeochake.com/blog/2018/lxc-troubleshooting/" rel="alternate" type="text/html" title="LXC Containers Not Authorized to Change Passwords"/><published>2018-05-17T00:00:00+00:00</published><updated>2018-05-17T00:00:00+00:00</updated><id>https://saurabhdeochake.com/blog/2018/lxc-troubleshooting</id><content type="html" xml:base="https://saurabhdeochake.com/blog/2018/lxc-troubleshooting/"><![CDATA[<p>This problem occurs especially when LXC Containers are run on a host machine with CentOS distribution. When we want to create an user inside the container, we get an error which says <code class="language-plaintext highlighter-rouge">..... is not authorized to change the password of &lt;user_name&gt;</code>.</p> <h1 id="the-problem">The Problem</h1> <p>This problem occurs especially when LXC Containers are run on a host machine with CentOS distribution. When we want to create an user inside the container, we get an error which says “….. is not authorized to change the password of user_name”. A snippet of the error is shown below:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[~]# passwd testuser

passwd: unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 is not authorized to change the password of testuser
</code></pre></div></div> <h1 id="the-solution">The Solution</h1> <p>The culprit behind this problem usually is SELinux. When set to “Enforcing” or “Permissive”, the SELinux would deny the user to change user parameters inside the containers like passwords as we encountered in the problem. Though, it is not safe to disable SELinux; we can solve above problem temporarily by checking the status of SELinux and disabling it. To get the “passwd” command work inside the container, follow the steps below:</p> <ol> <li>Check SELinux settings on CentOS Host Machine <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[~]$ sestatus
SELinux status:                 enabled
SELinuxfs mount:                /selinux
Current mode:                   enforcing
Mode from config file:          enforcing
Policy version:                 24
Policy from config file:        targeted
</code></pre></div> </div> </li> <li>Check SELinux settings on Guest <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[~]$ sestatus
SELinux status:                 enabled
SELinuxfs mount:                /selinux
Current mode:                   enforcing
</code></pre></div> </div> </li> <li>Disable SELinux from Config file <ul> <li>Go to the config file located at <code class="language-plaintext highlighter-rouge">/etc/selinux/config</code></li> <li>Change SELINUX option to disabled</li> <li>Save and close the file</li> </ul> </li> <li>Reboot the Host Machine</li> <li>After reboot, check SELinux setting parameter using getenforce <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[~]$ getenforce
Disabled
</code></pre></div> </div> </li> </ol> <p>Now, check again if the problem still persists in the guest virtual machine. This temporary fix should solve the problem.</p>]]></content><author><name></name></author><category term="troubleshooting"/><category term="cloud"/><category term="containers"/><category term="sre"/><category term="devops"/><category term="lxc"/><category term="troubleshooting"/><summary type="html"><![CDATA[How to fix the error - "..... is not authorized to change the password"]]></summary></entry><entry><title type="html">Cgroups and Containers</title><link href="https://saurabhdeochake.com/blog/2017/cgroups/" rel="alternate" type="text/html" title="Cgroups and Containers"/><published>2017-08-26T00:00:00+00:00</published><updated>2017-08-26T00:00:00+00:00</updated><id>https://saurabhdeochake.com/blog/2017/cgroups</id><content type="html" xml:base="https://saurabhdeochake.com/blog/2017/cgroups/"><![CDATA[<p>Cgroups is a Linux kernel feature to limit, account and isolate resource usage of process groups. Cgroups allow you to allocate resources—such as CPU time, system memory, network bandwidth, or combinations of these resources—among user-defined groups of tasks (processes) running on a system.</p> <h1 id="what-is-cgroups">What is Cgroups?</h1> <p>Cgroups is a Linux kernel feature to limit, account and isolate resource usage of process groups. Cgroups allow you to allocate resources—such as CPU time, system memory, network bandwidth, or combinations of these resources—among user-defined groups of tasks (processes) running on a system. You can monitor the cgroups you configure, deny cgroups access to certain resources, and even reconfigure your cgroups dynamically on a running system.<br/><br/><img src="/assets/img/cgroup.jpeg" width="500" height="300" align="center"/></p> <p><br/>The above figure describes the CPU shares limitation using Cgroups. We can see that three Cgroups use chunks of CPU. Cgroup #1’s share is 1024. Cgroup #2’s share is greater than both other Cgroups, so it’ll get more CPU than both others. Cgroup #3 will get least CPU share.</p> <h1 id="subsystems-of-cgroups">Subsystems of Cgroups:</h1> <p>Other than CPU subsystem, there are eight other subsystems available. Let’s have a look at all the Cgroups subsystems in brief:</p> <ul> <li>blkio — this subsystem sets limits on input/output access to and from block devices such as physical drives (disk, solid state, USB, etc.).</li> <li>cpu — this subsystem uses the scheduler to provide cgroup tasks access to the CPU.</li> <li>cpuacct — this subsystem generates automatic reports on CPU resources used by tasks in a cgroup.</li> <li>cpuset — this subsystem assigns individual CPUs (on a multicore system) and memory nodes to tasks in a cgroup.</li> <li>devices — this subsystem allows or denies access to devices by tasks in a cgroup.</li> <li>freezer — this subsystem suspends or resumes tasks in a cgroup.</li> <li>memory — this subsystem sets limits on memory use by tasks in a cgroup, and generates automatic reports on memory resources used by those tasks.</li> <li>net_cls — this subsystem tags network packets with a class identifier (classid) that allows the Linux traffic controller (tc) to identify packets originating from a particular cgroup task.</li> <li>net_prio — this subsystem provides a way to dynamically set the priority of network traffic per network interface.</li> <li>ns — the namespace subsystem.</li> </ul> <h1 id="features">Features:</h1> <p>Now, let’s discuss about features provided by Cgroups. Cgroups provides following features:</p> <ul> <li>Resource Limitation: Groups can be set to not exceed a resource limitation. This limitation includes memory limit, file system cache limit, disk I/O throughput limitation etc.</li> <li>Prioritization: Some groups may get a larger share of CPU, disk I/O throughput etc.</li> <li>Accounting: We can measure how much resources certain systems use.</li> <li>Isolation: Separate namespaces are provided for groups, so they remain fully exclusive to each other. The groups cannot see each other’s processes, network connections etc.</li> <li>Control: We can freeze groups, checkpoint and restart the container.</li> </ul> <p><br/> Cgroup come readily available with linux kernel source.</p>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="containers"/><category term="sre"/><category term="devops"/><category term="lxc"/><summary type="html"><![CDATA[What are cgroups?]]></summary></entry><entry><title type="html">Libvirt - a virtual machine management API</title><link href="https://saurabhdeochake.com/blog/2017/libvirt/" rel="alternate" type="text/html" title="Libvirt - a virtual machine management API"/><published>2017-08-26T00:00:00+00:00</published><updated>2017-08-26T00:00:00+00:00</updated><id>https://saurabhdeochake.com/blog/2017/libvirt</id><content type="html" xml:base="https://saurabhdeochake.com/blog/2017/libvirt/"><![CDATA[<p>In one of the previous posts, we discussed about lightweight virtualization solutions and more specifically, Linux Containers aka. LXC. Although, LXC has its own management tools that are shipped with the package, there are other APIs and virtual machine managers which can be used to manage various hypervisors and container systems. In this post, we discuss about Libvirt, a widely popular open source virtual machine management API.</p> <h1 id="what-is-libvirt">What is Libvirt?</h1> <p>Libvrit is an open source API, daemon and virtual machine manager for managing platform virtualization. Libvirt can be used to manage various virtualization solutions like LXC, KVM, OpenVZ, Xen and VMware ESX. Libvirt is a C Library, but it offers bindings in other languages some of which are Python,Perl, Ruby and Java. <br/><br/><img src="/assets/img/libvirt.jpeg" width="500" height="300" align="center"/></p> <h1 id="supported-hypervisors">Supported Hypervisors</h1> <p>Libvirt Supports a multitude of hypervisors and virtualization solutions. Some of widely known hypervisors supported by Libvirt are:</p> <ol> <li>LXC- Linux Containers</li> <li>OpenVZ</li> <li>QEMU/KVM</li> <li>Xen Hypervisor</li> <li>VirtualBox</li> <li>VMware ESX and GSX Hypervisors</li> <li>Microsoft’s Hyper-V Hypervisor</li> <li>IBM’s PowerVM for AIX</li> </ol> <h1 id="command-line-tools">Command Line Tools</h1> <p>Libvirt comes with a plethora of tools for management purposes. These tools range from virtual machine management to file system management, from memory management to provision of virtual machines over the network. Some of important command line tools that are used are:</p> <ul> <li><strong>virsh</strong>: An interactive shell and a virtual machine management tool which comes shipped with core distribution of Libvirt. It is one of the most important tool when it comes to managing various tasks on domains like virtual machines, containers or storage managed by Libvirt.</li> <li><strong>virsh-top</strong>: Another interactive command line utility to keep track of CPU, memory, disk and network utilization of all the virtual machines running on a host machine. This utility is essentially similar to “top” command which provides an ongoing look at processor activity of host machine.</li> <li><strong>virt-what</strong>: A shell script essentially used for detecting if the program is running on the virtual machine.</li> <li><strong>virt-df</strong>: Another commond line utility which is similar in working with that of Linux file system command, “df”. This utility offers the file system information of all virtual machine running on the host machine. This utility gives the information about how much disk space is used in each of guest disk.</li> <li><strong>virt-clone</strong>: This tool allows disk image(s) and configurations to be cloned from one virtual machine to another. This tool comes in handy especially while creating a new virtual machine from existing virtual machine keeping disk configurations intact. It automates copying of data across to new disk images, and updates the UUID, MAC address, and name in the configuration.</li> </ul> <h1 id="get-libvirt">Get Libvirt</h1> <p>Libvirt is an open source software and released under the GNU Lesser General Public License (see the file COPYING.LIB in the distribution package for the precise wording). You can get Libvirt from the latest upstream tarballs from here. You can also get it using git clone from <code class="language-plaintext highlighter-rouge">git clone git://libvirt.org/libvirt.git</code>.</p>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="containers"/><category term="sre"/><category term="devops"/><category term="lxc"/><summary type="html"><![CDATA[What is Libvirt?]]></summary></entry><entry><title type="html">Kubernetes on CentOS 7.0</title><link href="https://saurabhdeochake.com/blog/2017/kubernetes-centos/" rel="alternate" type="text/html" title="Kubernetes on CentOS 7.0"/><published>2017-07-24T00:00:00+00:00</published><updated>2017-07-24T00:00:00+00:00</updated><id>https://saurabhdeochake.com/blog/2017/kubernetes-centos</id><content type="html" xml:base="https://saurabhdeochake.com/blog/2017/kubernetes-centos/"><![CDATA[<h1 id="setting-up-kubernetes-on-centos-7">Setting up Kubernetes on CentOS 7</h1> <h3 id="1-introduction">1) Introduction</h3> <p>This document describes all the steps that are required to install and set up Kubernetes cluster on CentOS 7 machines. This set up tutorial is divided into three parts: first part discusses the lab machines set up that we follow, second part showcases installation steps required to set up Kubernetes cluster on CentOS 7 and finally third part discusses all the problems that may occur during the set up and potential fixes for them.</p> <h3 id="2-environment-setup">2) Environment Setup</h3> <p>Our set up consists of 3 bare metal machines which have CentOS 7 installed in them. You may also want to set up this cluster using virtual machines. We make sure that these three machines are on same subnet and they can ping each other over the network.</p> <p>So, below are our nodes including master node and worker nodes: <br/> 10.23.114.120 node1 – Master Node <br/> 10.23.114.194 node2 – Worker Node 1 <br/> 10.23.114.118 node3 – Worker Node 2</p> <p>Before you proceed to installation and setup steps next, please make sure you have made changes to <code class="language-plaintext highlighter-rouge">/etc/hosts</code> to include all the nodes by hostnames and IP. This will make sure we will have connectivity among machines by hostnames.</p> <h3 id="3-installation-and-setup">3) Installation and Setup</h3> <p>Kubernetes have their official guide for setting up Kubernetes cluster on CentOS 7 machines and it is located <a href="http://https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">here</a>. To execute steps below, you may want to acquire root user privileges. Now, if you are sitting behind a proxy, then you may want to add <code class="language-plaintext highlighter-rouge">http_proxy</code> variable in your system. You may also include the proxy variable by editing <code class="language-plaintext highlighter-rouge">/etc/yum.conf</code> file and adding “<code class="language-plaintext highlighter-rouge">proxy=&lt;http://proxy.something.com:port&gt;</code>” at the end of the file.</p> <ul> <li> <p><strong>Installation of Docker</strong> <br/> On each of your three machines- including master node and two worker nodes- install Docker. Please note that Kubernetes’ Kubeadm tool is not extensively tested on Docker versions 1.13 and 17.03+. Therefore, to avoid future problems with our set up we specifically install Docker version 1.12.</p> <ul> <li><strong>Add Docker Repo</strong></li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> yum-utils    
<span class="nv">$ </span>yum-config-manager <span class="se">\</span>
  <span class="nt">--add-repo</span> <span class="se">\</span>
  https://docs.docker.com/v1.13/engine/installation/linux/repo_files/centos/docker.repo
<span class="nv">$ </span>yum makecache fast   
</code></pre></div> </div> <p>There is a good chance that you might not be able to connect to docs.docker.com, so you may want to install docker.repo on your local windows machine first and then transfer that repo file to /etc/yum.repos.d/ directory in each of three machines using tools like WinSCP (on Windows) or create a new repo file.</p> <ul> <li><strong>List all packages</strong> <br/> Now, we check all versions of Docker that are offered in the repo we added above. As we know that Kubernetes kubeadm is not tested extensively on Docker 1.13 and 17.03+ versions, we install Docker 1.12 version.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>yum list docker-engine.x86_64  <span class="nt">--showduplicates</span> | <span class="nb">sort</span> –r

docker-engine.x86_64     1.13.1-1.el7.centos              docker-main   
docker-engine.x86_64     1.12.6-1.el7.centos              docker-main   
docker-engine.x86_64     1.11.2-1.el7.centos              docker-main   
</code></pre></div> </div> </li> </ul> <p>We will install Docker version 1.12 on all our machines in the setup.</p> <ul> <li><strong>Install and start Docker on all machines</strong> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>yum <span class="nb">install</span> <span class="nt">-y</span> docker-engine-1.12.6   
  <span class="nv">$ </span>systemctl start docker
  <span class="nv">$ </span>systemctl <span class="nb">enable </span>docker   
</code></pre></div> </div> </li> <li><strong>Install and Set Up Kubernetes Components</strong> <br/> Next, we will install core components of Kubernetes that are essential for our Kubernetes cluster to work. These components include kubectl, kubelet and kubeadm. We will also install other components that will be important for our set up. Install these components on all machines including master and worker nodes. <ul> <li><strong>Add Kubernetes Repo</strong></li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; /etc/yum.repos.d/kubernetes.repo

[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
      https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span class="no">EOF
</span></code></pre></div> </div> <p>Now, we check the version of packages offered by this repo.</p> <ul> <li><strong>List Packages</strong></li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>yum list kubeadm  <span class="nt">--showduplicates</span> |sort <span class="nt">-r</span>

kubeadm.x86_64            1.6.5-0                        kubernetes
kubeadm.x86_64            1.6.4-0                        kubernetes

<span class="nv">$ </span>yum list kubelet  <span class="nt">--showduplicates</span> |sort <span class="nt">-r</span>

kubelet.x86_64            1.6.5-0                        kubernetes
kubelet.x86_64            1.6.4-0                        kubernetes
kubelet.x86_64            1.6.3-0                        kubernetes

<span class="nv">$ </span>yum list kubectl  <span class="nt">--showduplicates</span> |sort <span class="nt">-r</span>

kubectl.x86_64            1.6.5-0                        kubernetes
kubectl.x86_64            1.6.4-0                        kubernetes
kubectl.x86_64            1.6.3-0                        kubernetes


<span class="nv">$ </span>yum list kubernetes-cni  <span class="nt">--showduplicates</span> |sort <span class="nt">-r</span>

kubernetes-cni.x86_64     0.5.1-0                        kubernetes
</code></pre></div> </div> <p>Next on, we install latest versions of these packages in following step.</p> <ul> <li><strong>Install <code class="language-plaintext highlighter-rouge">kubeadm</code>, <code class="language-plaintext highlighter-rouge">kubectl</code> and CNI Plugin</strong></li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>setenforce 0

<span class="nv">$ </span>yum <span class="nb">install</span> <span class="nt">-y</span> kubelet kubeadm kubectl kubernetes-cni
...
... 

Installed:
   kubeadm.x86_64 0:1.6.5-0              
       kubectl.x86_64 0:1.6.5-0
       kubelet.x86_64 0:1.6.5-0
       kubernetes-cni.x86_64 0:0.5.1-0

Complete!

<span class="nv">$ </span>systemctl <span class="nb">enable </span>kubelet.service
</code></pre></div> </div> <ul> <li><strong>Initialize Kubernetes Cluster</strong> <br/> Run following commands on Master node.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubeadm init <span class="nt">--kubernetes-version</span><span class="o">=</span>v1.6.5 <span class="nt">--pod-network</span> <span class="nv">cidr</span><span class="o">=</span>10.245.0.0/16 <span class="nt">--apiserver-advertise-address</span><span class="o">=</span>10.23.114.120
</code></pre></div> </div> <p>This initializes the cluster and our master node listens on address 10.23.114.120.</p> <ul> <li><strong>Start Using the Cluster</strong> <br/> To start using your cluster, please follow below commands as a regular user.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo cp</span> /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/
<span class="nv">$ </span><span class="nb">sudo chown</span> <span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span><span class="si">)</span>:<span class="si">$(</span><span class="nb">id</span> <span class="nt">-g</span><span class="si">)</span> <span class="nv">$HOME</span>/admin.conf
<span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="nv">$HOME</span>/admin.conf
</code></pre></div> </div> <p>Your Kubernetes cluster is good to go! Next step, we will deploy pods in our cluster.</p> <ul> <li> <p><strong>Deploy PODs</strong> <br/> Before we join worker nodes to the cluster, it is essential to set up CNI network configuration. We will use Flannel overlay network plugin for our networking. To avoid future problems with our pod, we will change the Flannel subnet address. Download yaml file from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml and in “Network” field change “10.244.0.0/16” to “10.245.0.0/16”. This is essential to prevent interesection of Flannel IP range with DNS range.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl apply –f https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml

serviceaccount <span class="s2">"flannel"</span> configured
configmap <span class="s2">"kube-flannel-cfg"</span> configured
daemonset <span class="s2">"kube-flannel-ds"</span> configured
</code></pre></div> </div> </li> <li> <p><strong>Apply Flannel RBAC Configuration</strong> To avoid “CrashLoopBackOff” error with our pods that we deployed earlier, we must apply yaml congurations related to Role Based Access Control (RBAC) authorization. Run following commands to apply RBAC.</p> </li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nv">$ </span>kubectl apply –f https://github.com/core    os/flannel/blob/master/Documentation/kube-flannel-rbac.yml 
   
 clusterrole <span class="s2">"flannel"</span> created
 clusterrolebinding <span class="s2">"flannel"</span> created
</code></pre></div> </div> <ul> <li><strong>Join Worker Nodes to Cluster</strong> <br/> After you start the cluster with kubeadm init command and the init command runs successfully, you should see a command to join the cluster using a token mentioned. Run that command as root on all the worker nodes so that they join the cluster.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubeadm <span class="nb">join</span> <span class="nt">--token</span> e7986d.e440de5882342711 10.23.114.120:6443
</code></pre></div> </div> <p>Now, to check if all nodes have joined the cluster please run following command on master node.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get nodes

NAME      STATUS     AGE       VERSION
node1     Ready      3m        v1.6.5
node2     Ready      2m        v1.6.5
node3     Ready      52s       v1.6.5
</code></pre></div> </div> <p>Your cluster is set up now and is good to go!</p> </li> </ul> <h3 id="4-problems-and-fixes">4) Problems and Fixes</h3> <p>In this section, we will mention various problem that you may encounter during the installation and setup of Kubernetes cluster.</p> <ul> <li> <p><strong>Problem</strong>: Kubeadm init stuck at “Waiting for the control plane to become ready” <br/> <strong>Fix</strong>: The cluster fails to initialize and is stuck at “waiting for the control plane to become ready”. The cluster is not waiting for services tied to control plane but it is stuck waiting because our Docker containers failed to launch. The root of the problem is that Docker tries to fetch images for containers but fails because it cannot go through the proxy. To work around this, please follow below steps to mention http_proxy variable for Docker environment.</p> <ul> <li>Create a systemd directory for Docker service</li> </ul> <p><code class="language-plaintext highlighter-rouge">$ mkdir /etc/systemd/system/docker.service.d</code></p> <ul> <li>Now, create an environment variable configuration file to hold our http proxy variable.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>vim /etc/systemd/system/docker.service.d/http-proxy.conf
	
  <span class="o">[</span>Service]
  <span class="nv">Environment</span><span class="o">=</span><span class="s2">"HTTP_PROXY=&lt;http://proxy.something.com:port&gt;"</span>
</code></pre></div> </div> <ul> <li>Flush changes and verify configuration <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span><span class="nb">sudo </span>systemctl daemon-reload
  <span class="nv">$ </span><span class="nb">sudo </span>systemctl show <span class="nt">--property</span> Environment docker
  <span class="nv">Environment</span><span class="o">=</span><span class="nv">HTTP_PROXY</span><span class="o">=</span><span class="nv">HTTP_PROXY</span><span class="o">=</span>&lt;http://proxy.something.com:port&gt;
  <span class="nv">$ </span>service docker restart
</code></pre></div> </div> </li> </ul> <p>If this does not work for you then there is another way it can be fixed:</p> <ul> <li>Open configuration file for Docker at /etc/sysconfig/docker and write proxy variable in the file.</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># vim /etc/sysconfig/docker</span>
  <span class="nv">HTTP_PROXY</span><span class="o">=</span><span class="nv">HTTP_PROXY</span><span class="o">=</span>&lt;http://proxy.something.com:port&gt;

  <span class="nv">$ </span>service docker restart
</code></pre></div> </div> </li> <li> <p><strong>Problem</strong>: Error message: <code class="language-plaintext highlighter-rouge">misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"</code> <br/> <strong>Fix</strong>: When you run kubectl get nodes to get all nodes in the cluster, you may not get all nodes listed in the output of the command.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>kubectl get nodes

  NAME      STATUS     AGE       VERSION
  node1     NotReady    3m        v1.6.5
</code></pre></div> </div> <p>I strongly suggest that you check journal logs for kubelet service to check what exactly is going on in the background. Run <code class="language-plaintext highlighter-rouge">journalctl –f –u kubelet.service</code>. You will see that kubelet service has failed due to error message- <code class="language-plaintext highlighter-rouge">misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"</code>.</p> <p>It is essential for normal operation that Kubernetes and Docker have same underlying cgroup driver for resources management. A quick fix for this problem is as follows:</p> <ul> <li>Open kubeadm configuration file “/etc/systemd/system/kubelet.service.d/10-kubeadm.conf” and edit the file to change cgroup driver for Kubernetes to match it with that of Docker. Locate “—cgroup-driver=systemd” and replace it with “—cgroup-driver=cgroupfs”</li> <li>Reload kubelet daemon with systemctl –deamon-reload</li> </ul> <p>You should see all your nodes in the cluster.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">$ </span>kubectl get nodes

  NAME      STATUS     AGE       VERSION
  node1     Ready      1m        v1.6.5
  node2     Ready      53m       v1.6.5
  node3     Ready      49s       v1.6.5
</code></pre></div> </div> </li> </ul> <hr/> <p>Have something to improve this setup guide? Feel free to add your suggestions/steps/text. Please maintain the formatting of this guide, if you plan to contribute to this document. Thank you!</p>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="kubernetes"/><category term="sre"/><category term="devops"/><summary type="html"><![CDATA[This is a thorough tutorial on setting up Kubernetes cluster on CentOS 7 servers. This is a step-by-step tutorial which also discusses potential problems that may be faced and how to solve them.]]></summary></entry><entry><title type="html">[Solved] Python Pynotify- gio.Error: The connection is closed</title><link href="https://saurabhdeochake.com/blog/2015/python-gio/" rel="alternate" type="text/html" title="[Solved] Python Pynotify- gio.Error: The connection is closed"/><published>2015-03-15T00:00:00+00:00</published><updated>2015-03-15T00:00:00+00:00</updated><id>https://saurabhdeochake.com/blog/2015/python-gio</id><content type="html" xml:base="https://saurabhdeochake.com/blog/2015/python-gio/"><![CDATA[<h1 id="pynotify">Pynotify</h1> <p>Pynotify is a Python package providing tools for implementing Observer programming pattern. These tools include signals, conditions and variables. Pynotify comes handy when one wants to write a script which outputs some “notification” pop-up on the desktop. Pynotify relies on Dbus IPC bus and problem occurred in Dbus may cause Pynotify to break. There is a well-known problem that usually occurs while using Pynotify. We discuss the problem below and solution to overcome the problem.</p> <h1 id="problem">Problem</h1> <p>Whenever we use show() method of pynotify to show the message in pop-up window, it gives you error in the following format:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Traceback </span><span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
  <span class="n">File</span> <span class="sh">"</span><span class="s">NotifyMe.py</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">14</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
    <span class="n">notifications</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">gio</span><span class="p">.</span><span class="n">Error</span><span class="p">:</span> <span class="n">Connection</span> <span class="n">Closed</span>
</code></pre></div></div> <p>The gio.Error is related to Dbus suggesting that the connection to Dbus is closed.</p> <h1 id="cause">Cause</h1> <p>I was working on one project which uses Pynotify and I encountered the same problem. After a lot of research I found out that I was calling my script with root user. If you are also running your script with root user, then it may be the problem. The problem is that root does not have a dbus session running. I found out that it does not even own XScreen. I assume we want to use Dbus session that belongs to the logged in user.</p> <h1 id="solution">Solution</h1> <p>There are two or three workarounds to tackle the problem and get your pynotify script working. We discuss those solutions below:</p> <ul> <li><strong><em>Quit Root User Session</em></strong>: Most probably current Dbus session belongs to normal user which is currently logged in. So, better and simple way to avoid above problem is to quit root user session using “ctrl+d” and run the script again.</li> <li><strong><em>Use GKsu</em></strong>: GKSu is a library that provides a Gtk+ frontend to su and sudo. GUI applications should be started with gksu, not su or sudo.</li> <li><strong><em>Add “root” User to Dbus Group</em></strong>: A simple workaround can be adding the “root” user to Dbus group to give it the permission to connect to Dbus. I have not verified this step. So, if it works for you, please let me know in the comments section.</li> <li><strong><em>Exporting Display Variable</em></strong>: Another workaround that MAY help in solving the problem is exporting display variable. Add following lines of code to start of your python script and see if it helps. <br/> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">os</span>
  <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">DISPLAY</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">:0.0</span><span class="sh">'</span>
</code></pre></div> </div> </li> </ul> <p>I hope above solutions will work for you and you’d be able to work with pynotify in your projects.</p>]]></content><author><name></name></author><category term="troubleshooting"/><category term="python"/><category term="programming"/><category term="troubleshooting"/><summary type="html"><![CDATA[This post discusses the problem of "gio.Error: Connection Closed" error in Pynotify, its causes and steps to solve the problem.]]></summary></entry><entry><title type="html">LXC - In a Nutshell</title><link href="https://saurabhdeochake.com/blog/2014/lxc/" rel="alternate" type="text/html" title="LXC - In a Nutshell"/><published>2014-02-14T00:00:00+00:00</published><updated>2014-02-14T00:00:00+00:00</updated><id>https://saurabhdeochake.com/blog/2014/lxc</id><content type="html" xml:base="https://saurabhdeochake.com/blog/2014/lxc/"><![CDATA[<p>Before the emergence of container technologies like Docker and Rkt, LXC was one of the main container technology that was being in use. This post discusses LXC in a nutshell. Learn more about LXC, its architecture and components.</p> <h1 id="what-is-lxc">What is LXC?</h1> <p>LXC (LinuX Containers) is an operating system-level virtualization method for running multiple isolated Linux systems (containers) on a single control host. LXC provides operating system-level virtualization not via a full blown virtual machine, but rather provides a virtual environment that has its own process and network space. LXC is an userspace API which supports Linux Kernel’s support for containment. With containers, you have the option of kicking off any individual process you like inside any container. Instead, what we do is set up a file system containing a copy of a minimal operating system image, and kick off /sbin/init on that partition, in that file system, with its own Ethernet interface. We get the appearance of a VM, each container getting its own IP address, and disk file systems, its own set of software packages installed, and its own set of OS daemons processes.<br/><br/><img src="/assets/img/lxc.png" width="500" height="300" align="middle"/></p> <h1 id="features">Features</h1> <p>LXC provides following features of Linux Kernel to support containment:</p> <ul> <li>Chroot</li> <li>Control groups (cgroups)</li> <li>Kernel namespaces (ipc, uts, mount, pid, network and user)</li> <li>Apparmor and SELinux</li> <li>Kernel capabilities and many more…</li> </ul> <h1 id="chroot-and-lxc">Chroot and LXC</h1> <p>In simple words, LXC builds up from chroot to implement complete virtual systems, adding resource management and isolation mechanisms to Linux’s existing process management infrastructure. LXC is often considered as a virtualization solution between chroot on steroids and complete full blown virtual machine. The main goal of LXC is to create a closest possible environment as that of a standard Linux installation but without the need for a separate kernel.</p> <h1 id="container-life-cycle">Container Life Cycle</h1> <p>When the container is created, it contains the configuration information. When a process is launched, the container will be starting and running. When the last process running inside the container exits, the container is stopped. In case of failure when the container is initialized, it will pass through the aborting state.</p> <h1 id="container-management">Container Management</h1> <p>The containers can be managed with the help of some popular container-management utilites like lxc-tools and other libraries like libvirt. Libvirt is an open source API, daemon and management tool for managing platform virtualization.</p> <h1 id="get-lxc">Get LXC</h1> <p>LXC is free software and is released under the terms of the GNU LGPLv2.1+ license. You can get it using git clone from <code class="language-plaintext highlighter-rouge">git clone git://github.com/lxc/lxc</code></p>]]></content><author><name></name></author><category term="containers"/><category term="cloud"/><category term="lxc"/><category term="virtualization"/><category term="containers"/><summary type="html"><![CDATA[march & april, looking forward to summer]]></summary></entry></feed>